# -*- coding: utf-8 -*-
"""Final Submission_Muhammad Rizq Ramadhan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UUOX7e8fe_cb_jUmKOz3l7qLRUAFZTGN
"""

import os
from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import matplotlib.pyplot as plt

pip install split-folders

import splitfolders

bus_dir = os.path.join('/content/drive/MyDrive/dataset/bus')
plane_dir = os.path.join('/content/drive/MyDrive/dataset/plane')
ship_dir = os.path.join('/content/drive/MyDrive/dataset/ship')

bus = os.listdir(bus_dir)
plane = os.listdir(plane_dir)
ship = os.listdir(ship_dir)

print('total training bus images:', len(bus))
print('total training plane images:', len(plane))
print('total training ship images:', len(ship))

print('total dataset: ', len(bus) + len(plane) + len(ship))

base_dir = '/content/drive/MyDrive/dataset'
splitfolders.ratio(base_dir, output=f'{base_dir}/split', seed=42, ratio=(0.8, 0.2), group_prefix=None)

model = Sequential([
    Conv2D(16, (3,3), activation='relu', input_shape=(300,300,3)),
    MaxPooling2D(2,2),
    Conv2D(32, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Dropout(0.2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Dropout(0.5),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(3, activation='softmax')
])

model.summary()

train_datagen = ImageDataGenerator(
    rescale=1/255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

val_datagen = ImageDataGenerator(rescale=1/255)

train = '/content/drive/MyDrive/dataset/split/train'
val = '/content/drive/MyDrive/dataset/split/val'

train_generator = train_datagen.flow_from_directory(
    train,
    target_size=(300,300),
    batch_size=32,
    class_mode='categorical'
)

validation_generator = val_datagen.flow_from_directory(
    val,
    target_size=(300,300),
    batch_size=32,
    class_mode='categorical'
)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if (logs.get('accuracy') > 0.85 and logs.get('val_accuracy') > 0.85):
      print("\nAkurasi telah melebihi 85%!")
      self.model.stop_training = True

callbacks = myCallback()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(
    train_generator,
    steps_per_epoch=32,
    epochs=50,
    batch_size=70,
    validation_data=validation_generator,
    validation_steps=8,
    verbose=1,
    callbacks=[callbacks]
)

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Metrics')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Loss Accuracy')
plt.plot(history.history['val_loss'], label='Loss Validation')
plt.title('Loss Metrics')
plt.legend()
plt.show()

export_dir = "/content/drive/MyDrive/dataset"
tf.saved_model.save(model, export_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

with tf.io.gfile.GFile('submission.tflite', 'wb') as f:
    f.write(tflite_model)